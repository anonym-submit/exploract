{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from category_encoders import one_hot\n",
    "# import cudf\n",
    "# from cuml.decomposition import PCA as cumlPCA\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utilities import Repository\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repo = Repository('./session_repositories/actions.tsv','./session_repositories/displays.tsv','./raw_datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mac_to_int(mac):\n",
    "#     res = re.match('^((?:(?:[0-9a-f]{2}):){5}[0-9a-f]{2})$', mac.lower())\n",
    "#     if res is None:\n",
    "#         raise ValueError('invalid mac address')\n",
    "#     # return int(res.group(0).replace(':', ''), 16)\n",
    "#     return ','.join([str(int(x, 16)) for x in res.group(0).split(\":\")])\n",
    "\n",
    "def mac_seperation(mac):\n",
    "    res = re.match('^((?:(?:[0-9a-f]{2}):){5}[0-9a-f]{2})$', mac.lower())\n",
    "    if res is None:\n",
    "        raise ValueError('invalid mac address')\n",
    "    # print(res.group(0))\n",
    "    return ','.join([res.group(0)[:8], res.group(0)[9:]])\n",
    "\n",
    "def timestamp_decompose(timestamp):\n",
    "    timestamp_obj = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    return f'{timestamp_obj.year},{timestamp_obj.month},{timestamp_obj.day},{timestamp_obj.hour},{timestamp_obj.minute},{timestamp_obj.second}'\n",
    "\n",
    "def alter_df_columns(df, drop_pid=True):\n",
    "    if 'ip_dst' in df.columns:\n",
    "        df[['ip_dst_0', 'ip_dst_1', 'ip_dst_2', 'ip_dst_3']] = df['ip_dst'].fillna('x.x.x.x').str.split('.', expand=True, n=4).replace('x', np.nan).astype('float32')\n",
    "        df = df.drop(columns=['ip_dst'])\n",
    "\n",
    "    if 'ip_dst_count' in df.columns:\n",
    "        df = df.assign(**{x: df['ip_dst_count'] for x in ['ip_dst_0_count', 'ip_dst_1_count', 'ip_dst_2_count', 'ip_dst_3_count']})\n",
    "        df = df.drop(columns=['ip_dst_count'])\n",
    "        \n",
    "    if 'ip_src' in df.columns:\n",
    "        df[['ip_src_0', 'ip_src_1', 'ip_src_2', 'ip_src_3']] = df['ip_src'].fillna('x.x.x.x').str.split('.', expand=True, n=4).replace('x', np.nan).astype('float32')\n",
    "        df = df.drop(columns=['ip_src'])\n",
    "\n",
    "    if 'ip_src_count' in df.columns:\n",
    "        df = df.assign(**{x: df['ip_src_count'] for x in ['ip_src_0_count', 'ip_src_1_count', 'ip_src_2_count', 'ip_src_3_count']})\n",
    "        df = df.drop(columns=['ip_src_count'])\n",
    "\n",
    "    if 'eth_dst' in df.columns:\n",
    "        df['eth_dst_tokens'] = df.apply(lambda x: mac_seperation(x.eth_dst), axis=1)\n",
    "        df[['eth_dst_oui', 'eth_dst_di']] = df['eth_dst_tokens'].str.split(',', expand=True, n=2)\n",
    "        df = df.drop(columns=['eth_dst', 'eth_dst_tokens'])\n",
    "\n",
    "    if 'eth_dst_count' in df.columns:\n",
    "        df = df.assign(**{x: df['eth_dst_count'] for x in ['eth_dst_oui_count', 'eth_dst_di_count']})\n",
    "        df = df.drop(columns=['eth_dst_count'])\n",
    "\n",
    "    if 'eth_src' in df.columns:\n",
    "        df['eth_src_tokens'] = df.apply(lambda x: mac_seperation(x.eth_src), axis=1)\n",
    "        df[['eth_src_oui', 'eth_src_di']] = df['eth_src_tokens'].str.split(',', expand=True, n=2)\n",
    "        df = df.drop(columns=['eth_src', 'eth_src_tokens'])\n",
    "\n",
    "    if 'eth_src_count' in df.columns:\n",
    "        df = df.assign(**{x: df['eth_src_count'] for x in ['eth_src_oui_count', 'eth_src_di_count']})\n",
    "        df = df.drop(columns=['eth_src_count'])\n",
    "\n",
    "    if 'sniff_timestamp' in df.columns:\n",
    "        df['timestamp_tokens'] = df.apply(lambda x: timestamp_decompose(x.sniff_timestamp), axis=1)\n",
    "        df[['year', 'month', 'day', 'hour', 'minutes', 'seconds']] = df['timestamp_tokens'].str.split(',', expand=True, n=6).astype('float32')\n",
    "        df = df.drop(columns=['sniff_timestamp', 'timestamp_tokens'])\n",
    "\n",
    "    if 'sniff_timestamp_count' in df.columns:\n",
    "        df = df.assign(**{x: df['sniff_timestamp_count'] for x in ['year_count', 'month_count', 'day_count', 'hour_count', 'minutes_count', 'seconds_count']})\n",
    "        df = df.drop(columns=['sniff_timestamp_count'])\n",
    "\n",
    "    if 'interface_captured' in df.columns:\n",
    "        df = df.drop(columns=['interface_captured'])\n",
    "\n",
    "    if drop_pid:\n",
    "        if 'project_id' in df.columns:\n",
    "            df = df.drop(columns=['project_id'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_numbers_and_uris(string):\n",
    "    # Replace IP addresses with <IP>\n",
    "    string = re.sub(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\", \"<IP>\", string)\n",
    "    # Replace MAC addresses with <MAC>\n",
    "    string = re.sub(r\"\\b([0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}\\b\", \"<MAC>\", string)\n",
    "    \n",
    "    # Replace numbers (decimal and hex) except IP and MAC addresses\n",
    "    string = re.sub(r\"\\b(0x[a-fA-F0-9]+|\\d+)\\b\", \"<NUMBER>\", string)\n",
    "    \n",
    "    # Replace only the URI part after HTTP request methods (GET, POST, etc.)\n",
    "    uri_pattern = r\"\\b(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH)\\s+([^\\s]+)\"\n",
    "    string = re.sub(uri_pattern, r\"\\1 <URI>\", string)\n",
    "    # Replace other general URIs (ftp, file, mailto, etc.)\n",
    "    string = re.sub(r\"\\b(?:[a-zA-Z][a-zA-Z\\d+.-]*):\\/\\/[^\\s]+(?:\\s+[^\\s]+)*\", \"<URI>\", string)\n",
    "    \n",
    "    \n",
    "    # Replace domain names with <DOMAIN>\n",
    "    string = re.sub(r\"\\b(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}\\b\", \"<DOMAIN>\", string)\n",
    "    return string\n",
    "\n",
    "def extract_templates(strings):\n",
    "    templates = [replace_numbers_and_uris(s) for s in strings]\n",
    "    unique_templates = set(templates)\n",
    "    return unique_templates\n",
    "\n",
    "def identify_template(new_string, templates):\n",
    "    processed_new_string = replace_numbers_and_uris(new_string)\n",
    "    if processed_new_string in templates:\n",
    "        return processed_new_string\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     print(repo.data[i].shape)\n",
    "#     print()\n",
    "\n",
    "#     for col in repo.data[i].columns:\n",
    "#         print(col, repo.data[i][col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(4):\n",
    "    df_list.append(repo.data[i])\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "print(df.shape)\n",
    "\n",
    "df = alter_df_columns(df, drop_pid=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['ip_dst_0', 'ip_dst_1', 'ip_dst_2', 'ip_dst_3']] = df['ip_dst'].str.split('.', expand=True, n=4).astype('float32')\n",
    "# df[['ip_src_0', 'ip_src_1', 'ip_src_2', 'ip_src_3']] = df['ip_src'].str.split('.', expand=True, n=4).astype('float32')\n",
    "\n",
    "# df['eth_dst_tokens'] = df.apply(lambda x: mac_seperation(x.eth_dst), axis=1)\n",
    "# df[['eth_dst_oui', 'eth_dst_di']] = df['eth_dst_tokens'].str.split(',', expand=True, n=2)\n",
    "# df['eth_src_tokens'] = df.apply(lambda x: mac_seperation(x.eth_src), axis=1)\n",
    "# df[['eth_src_oui', 'eth_src_di']] = df['eth_src_tokens'].str.split(',', expand=True, n=2)\n",
    "# df['timestamp_tokens'] = df.apply(lambda x: timestamp_decompose(x.sniff_timestamp), axis=1)\n",
    "# df[['year', 'month', 'day', 'hour', 'minutes', 'seconds']] = df['timestamp_tokens'].str.split(',', expand=True, n=6).astype('float32')\n",
    "# df = df.drop(columns=['ip_dst', 'ip_src', 'sniff_timestamp', 'timestamp_tokens', 'eth_dst', 'eth_src', 'eth_src_tokens', 'eth_dst_tokens', 'interface_captured'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = []\n",
    "\n",
    "for line in df['info_line']:\n",
    "    strings.append(line)\n",
    "\n",
    "# Find common templates\n",
    "templates = extract_templates(strings)\n",
    "# for template in templates:\n",
    "#     print(template)\n",
    "#     print('-----------------------------------------------------------------------------')\n",
    "\n",
    "templates = list(templates)\n",
    "templates.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoric = ['highest_layer', 'eth_dst', 'eth_src', 'ip_dst', 'ip_src']\n",
    "# # categoric = ['eth_dst', 'eth_src', 'highest_layer']\n",
    "# numeric = ['captured_length', 'length', 'number', \n",
    "#             'tcp_dstport', 'tcp_srcport', 'tcp_stream', 'year', 'month', 'day', 'hour', 'minutes', 'seconds']\n",
    "# # numeric = ['ip_dst_0', 'ip_dst_1', 'ip_dst_2', 'ip_dst_3', 'ip_src_0', 'ip_src_1', 'ip_src_2', 'ip_src_3', 'length', 'number', 'tcp_dstport', 'tcp_srcport', 'tcp_stream']\n",
    "# id = ['project_id']\n",
    "# text = ['info_line']\n",
    "# # time = ['sniff_timestamp_epoch']\n",
    "\n",
    "# # finally divide the dataset using 'project_id' = 1,2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric = ['eth_dst_oui', 'eth_dst_di', 'eth_src_oui', 'eth_src_di', \n",
    "             'ip_dst_0', 'ip_dst_1', 'ip_dst_2', 'ip_dst_3', \n",
    "             'ip_src_0', 'ip_src_1', 'ip_src_2', 'ip_src_3', \n",
    "             'highest_layer', 'info_line', \n",
    "             'tcp_dstport', 'tcp_srcport', \n",
    "             'year', 'month', 'day', 'hour', 'minutes', 'seconds']\n",
    "# categoric = ['eth_dst', 'eth_src', 'highest_layer']\n",
    "numeric = ['captured_length', 'length', 'tcp_stream']\n",
    "# numeric = ['ip_dst_0', 'ip_dst_1', 'ip_dst_2', 'ip_dst_3', 'ip_src_0', 'ip_src_1', 'ip_src_2', 'ip_src_3', 'length', 'number', 'tcp_dstport', 'tcp_srcport', 'tcp_stream']\n",
    "id = ['project_id']\n",
    "number = ['number']\n",
    "# time = ['sniff_timestamp_epoch']\n",
    "\n",
    "# finally divide the dataset using 'project_id' = 1,2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_numeric = df[numeric].copy()\n",
    "# df_categoric = df[categoric].copy().astype(dtype='object')\n",
    "# df_id = df[id].copy()\n",
    "# df_number = df[number].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_types = ['count', 'min', 'max', 'sum', 'avg']\n",
    "\n",
    "num_intervals = 100\n",
    "numeric_col_ranges = {}\n",
    "for col in numeric:\n",
    "    col_min = df[col].min().__floor__()\n",
    "    col_max = df[col].max().__ceil__()\n",
    "    \n",
    "    col_count = 0 #len(df)\n",
    "    col_sum = 0 #df[col].sum().astype('float64').__ceil__()\n",
    "    for pid in range(1, 5):\n",
    "        pid_df = df[df['project_id'] == pid][col]\n",
    "        pid_sum = pid_df.sum().astype('float64').__ceil__()\n",
    "        if pid_sum > col_sum:\n",
    "            col_sum = pid_sum\n",
    "        if len(pid_df) > col_count:\n",
    "            col_count = len(pid_df)\n",
    "\n",
    "    col_avg = df[col].mean().__ceil__()\n",
    "\n",
    "    interval = math.ceil(col_max / num_intervals)\n",
    "    numeric_col_ranges[col] = (0, col_max, interval)\n",
    "    \n",
    "    for agg_type in agg_types:\n",
    "        agg_col = f'{col}_{type}'\n",
    "        if agg_type in ['min', 'max', 'avg']:\n",
    "            interval = col_max / num_intervals\n",
    "            numeric_col_ranges[agg_col] = (0, col_max, math.ceil(interval))\n",
    "        elif agg_type == 'sum':\n",
    "            interval = col_sum / num_intervals\n",
    "            numeric_col_ranges[agg_col] = (0, col_sum, math.ceil(interval))\n",
    "        elif agg_type == 'count':\n",
    "            interval = col_count / num_intervals\n",
    "            numeric_col_ranges[agg_col] = (0, col_count, math.ceil(interval))\n",
    "\n",
    "categoric_col_ranges = {}\n",
    "for col in categoric:\n",
    "    col_max = len(df)\n",
    "    interval = math.ceil(col_max / num_intervals)\n",
    "    categoric_col_ranges[f'{col}_count'] = (0, col_max, interval)\n",
    "\n",
    "number_col_ranges = {}\n",
    "for col in number:\n",
    "    col_max = len(df)\n",
    "    interval = math.ceil(col_max / num_intervals)\n",
    "    number_col_ranges[f'{col}_count'] = (0, col_max, interval)\n",
    "\n",
    "categoric_col_uniques = {}\n",
    "for col in categoric:\n",
    "    if col == 'info_line':\n",
    "        categoric_col_uniques[col] = {templates[i]: i for i in range(len(templates))}\n",
    "        categoric_col_uniques[col]['nan'] = len(templates)\n",
    "    else:\n",
    "        col_uniques = df[col].dropna().unique().tolist()\n",
    "        col_uniques.sort()\n",
    "        categoric_col_uniques[col] = {col_uniques[i]: i for i in range(len(col_uniques))}\n",
    "        categoric_col_uniques[col]['nan'] = len(col_uniques)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(qdf, col_bins):\n",
    "    col_bins = copy.deepcopy(col_bins)\n",
    "    for col in qdf.columns:\n",
    "        if col in numeric_col_ranges.keys():\n",
    "            ### handing the continuous\n",
    "            for value in qdf[col]:\n",
    "                if pd.isna(value):\n",
    "                    index = -1\n",
    "                else:\n",
    "                    index = math.floor(value / numeric_col_ranges[col][2])\n",
    "                col_bins[col][index] += 1\n",
    "        elif col in categoric_col_uniques:\n",
    "            ### handling categoric\n",
    "            if col == 'info_line':\n",
    "                for value in qdf[col]:\n",
    "                    processed_new_string = replace_numbers_and_uris(value)\n",
    "                    index = categoric_col_uniques[col]['nan'] ## index corresponding to unmatching strings and nans\n",
    "                    if processed_new_string in categoric_col_uniques[col].keys():\n",
    "                        index = categoric_col_uniques[col][processed_new_string]\n",
    "                    col_bins[col][index] += 1\n",
    "            else:\n",
    "                for value in qdf[col]:\n",
    "                    if pd.isna(value):\n",
    "                        index = -1\n",
    "                    else:\n",
    "                        index = categoric_col_uniques[col][value]\n",
    "                    col_bins[col][index] += 1\n",
    "        elif col in categoric_col_ranges.keys():\n",
    "            ### handling the categoric_count\n",
    "            for value in qdf[col]:\n",
    "                index = math.floor(value / categoric_col_ranges[col][2])\n",
    "                col_bins[col][index] += 1\n",
    "        elif col in number_col_ranges.keys():\n",
    "            ### handling number_count\n",
    "            for value in qdf[col]:\n",
    "                index = math.floor(value / number_col_ranges[col][2])\n",
    "                col_bins[col][index] += 1\n",
    "\n",
    "    for col in col_bins:\n",
    "        total = sum(col_bins[col])\n",
    "        col_bins[col] = np.array(col_bins[col], dtype=np.float32)\n",
    "        if total > 0:\n",
    "            col_bins[col] = col_bins[col] / total\n",
    "\n",
    "    return col_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_bins = {}\n",
    "\n",
    "for col in numeric_col_ranges:\n",
    "    col_bins[col] = [0] * (num_intervals + 1)\n",
    "\n",
    "for col in categoric_col_ranges:\n",
    "    col_bins[col] = [0] * num_intervals\n",
    "\n",
    "for col in number_col_ranges:\n",
    "    col_bins[col] = [0] * num_intervals\n",
    "\n",
    "for col in categoric_col_uniques:\n",
    "    col_bins[col] = [0] * len(categoric_col_uniques[col])\n",
    "\n",
    "feats_order = list(col_bins.keys())\n",
    "feats_order.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_error_displays = [427, 428, 429, 430, \n",
    "                        854, 855, 856, 868, 891, \n",
    "                        977, 978, 979, 980, \n",
    "                        1304, 1908, 1909, 1983, \n",
    "                        2022, 2023, 2024, 2195,\n",
    "                        3244, 3446, 3447, \n",
    "                        4050, 4051, 4056, 4052, 4054, 4055, 4057, 4058, 4059, \n",
    "                        4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_feats = {}\n",
    "for tup in repo.displays.itertuples():\n",
    "    if tup.display_id in logic_error_displays:\n",
    "        continue\n",
    "\n",
    "    grouping = json.loads(tup.grouping)\n",
    "    aggregation = json.loads(tup.aggregations)\n",
    "\n",
    "    try:\n",
    "        df_bins = None\n",
    "        if aggregation:\n",
    "            # print(tup.display_id, grouping, aggregation)\n",
    "            raw_df, grp_df = repo.get_raw_display(tup.display_id)\n",
    "\n",
    "            if grp_df.empty:\n",
    "                df_bins = copy.deepcopy(col_bins)\n",
    "                for col in df_bins:\n",
    "                    df_bins[col] = np.array(df_bins[col], dtype=np.float32)\n",
    "            else:\n",
    "                for col_and_type in aggregation['list']:\n",
    "                    col = f\"{col_and_type['field']}_{col_and_type['type']}\"\n",
    "                    grp_df.rename(columns={col_and_type['field']: col}, inplace=True)\n",
    "                grp_df.rename(columns={'number': 'number_count'}, inplace=True)\n",
    "\n",
    "                grp_df.reset_index(allow_duplicates=True, inplace=True)\n",
    "                grp_df = grp_df.loc[:,~grp_df.columns.duplicated()].copy()\n",
    "\n",
    "                grp_df = alter_df_columns(grp_df)\n",
    "                df_bins = get_distribution(grp_df, col_bins)\n",
    "\n",
    "            # print(grp_df.head())\n",
    "            # break\n",
    "        elif len(grouping['list']) > 0:\n",
    "            # print(tup.display_id, grouping, aggregation)\n",
    "            raw_df, grp_df = repo.get_raw_display(tup.display_id)\n",
    "\n",
    "            if grp_df.empty:\n",
    "                df_bins = copy.deepcopy(col_bins)\n",
    "                for col in df_bins:\n",
    "                    df_bins[col] = np.array(df_bins[col], dtype=np.float32)\n",
    "            else:\n",
    "                grp_df.rename(columns={'number': 'number_count'}, inplace=True)\n",
    "\n",
    "                grp_df.reset_index(allow_duplicates=True, inplace=True)\n",
    "                grp_df = grp_df.loc[:,~grp_df.columns.duplicated()].copy()\n",
    "\n",
    "                grp_df = alter_df_columns(grp_df)\n",
    "                df_bins = get_distribution(grp_df, col_bins)\n",
    "\n",
    "            # print(grp_df.head())\n",
    "        else:\n",
    "            # print(tup.display_id, tup.filtering)\n",
    "            raw_df, grp_df = repo.get_raw_display(tup.display_id)\n",
    "\n",
    "            if raw_df.empty:\n",
    "                df_bins = copy.deepcopy(col_bins)\n",
    "                for col in df_bins:\n",
    "                    df_bins[col] = np.array(df_bins[col], dtype=np.float32)\n",
    "            else:\n",
    "                raw_df = alter_df_columns(raw_df)\n",
    "                df_bins = get_distribution(raw_df, col_bins)\n",
    "\n",
    "            # print(raw_df.head())\n",
    "\n",
    "        if df_bins:\n",
    "            to_concat = []\n",
    "            for col in feats_order:\n",
    "                to_concat.append(df_bins[col])\n",
    "            display_feats[tup.display_id] = np.concatenate(to_concat).copy()\n",
    "            # display_feats[tup.display_id] = to_concat\n",
    "\n",
    "            with open(f'./display_feats/display_feats_new.pickle', 'wb') as fout:\n",
    "                pickle.dump(display_feats, fout, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(tup.display_id, tup.filtering, grouping, aggregation)\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./display_feats/display_feats_new.pickle', 'rb') as fin:\n",
    "    display_feats = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_for_pca = []\n",
    "for d in display_feats:\n",
    "    feats_for_pca.append(display_feats[d])\n",
    "feats_for_pca = np.array(feats_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 4\n",
    "feats_for_pca = []\n",
    "for i, row in repo.displays[repo.displays['project_id'] == pid].iterrows():\n",
    "    did = row['display_id']\n",
    "    if not (did in logic_error_displays):\n",
    "        feats_for_pca.append(display_feats[did])\n",
    "feats_for_pca = np.array(feats_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_pca = PCA(n_components=feats_for_pca.shape[0])\n",
    "big_pca.fit(feats_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_var = 0.9999\n",
    "total_evr = 0.0\n",
    "components = 0\n",
    "for evr in big_pca.explained_variance_ratio_:\n",
    "    total_evr += evr\n",
    "    components += 1\n",
    "    if total_evr >= req_var:\n",
    "        break\n",
    "\n",
    "print(total_evr, components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=components)\n",
    "pca.fit(feats_for_pca)\n",
    "pca_scaler = MinMaxScaler()\n",
    "pca_scaler.fit(pca.transform(feats_for_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_feats = {}\n",
    "for i, row in repo.displays[repo.displays['project_id'] == pid].iterrows():\n",
    "    did = row['display_id']\n",
    "    if not (did in logic_error_displays):\n",
    "        display_pca_feats[did] = pca_scaler.transform(pca.transform([display_feats[did]]))[0]\n",
    "\n",
    "with open(f'./display_feats/display_pca_feats_{int(req_var * 1e4)}_{pid}.pickle', 'wb') as fout:\n",
    "    pickle.dump(display_pca_feats, fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_feats = {}\n",
    "for d in display_feats:\n",
    "    display_pca_feats[d] = pca_scaler.transform(pca.transform([display_feats[d]]))[0]\n",
    "\n",
    "with open(f'./display_feats/display_pca_feats_{int(req_var * 1e4)}.pickle', 'wb') as fout:\n",
    "    pickle.dump(display_pca_feats, fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_columns = ['captured_length', 'length', 'tcp_stream', 'number', 'eth_dst', 'eth_src', \n",
    "                'highest_layer', 'info_line', 'interface_captured', 'ip_dst', 'ip_src', \n",
    "                'sniff_timestamp', 'tcp_dstport', 'tcp_srcport']\n",
    "\n",
    "logic_error_displays = [427, 428, 429, 430, \n",
    "                        854, 855, 856, 868, 891, \n",
    "                        977, 978, 979, 980, \n",
    "                        1304, 1908, 1909, 1983, \n",
    "                        2022, 2023, 2024, 2195,\n",
    "                        3244, 3446, 3447, \n",
    "                        4050, 4051, 4056, 4052, 4054, 4055, 4057, 4058, 4059, \n",
    "                        4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067]\n",
    "\n",
    "for i, row in repo.actions.iterrows():\n",
    "    u = int(row['parent_display_id'])\n",
    "    v = int(row['child_display_id'])\n",
    "    aid = int(row['action_id'])\n",
    "    action_type = row['action_type']\n",
    "    action_params = row['action_params']\n",
    "\n",
    "    if row['action_type'] == 'sort' and (not bool(row['action_params'])):\n",
    "        check_col = 'number'\n",
    "    else:\n",
    "        check_col = row['action_params']['field']\n",
    "\n",
    "    if not check_col in og_columns:\n",
    "        continue\n",
    "\n",
    "    if (not u in display_feats) and (not u in logic_error_displays):\n",
    "        print('child', u, aid)\n",
    "        \n",
    "    \n",
    "    # if (not v in display_feats) and (not v in logic_error_displays):\n",
    "    #     print('parent', v, aid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IGTD.Scripts.IGTD_Functions import min_max_transform, table_to_image, select_features_by_variation, multi_table_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./display_feats/display_feats.pickle', 'rb') as fin:\n",
    "    display_feats = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 30    # Number of pixel rows in image representation\n",
    "num_col = 30    # Number of pixel columns in image representation\n",
    "num = num_row * num_col # Number of features to be included for analysis, which is also the total number of pixels in image representation\n",
    "save_image_size = 3 # Size of pictures (in inches) saved during the execution of IGTD algorithm.\n",
    "max_step = 30000    # The maximum number of iterations to run the IGTD algorithm, if it does not converge.\n",
    "val_step = 300  # The number of iterations for determining algorithm convergence. If the error reduction rate\n",
    "                # is smaller than a pre-set threshold for val_step itertions, the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_for_df = []\n",
    "d_order = []\n",
    "for d in display_feats:\n",
    "    feats_for_df.append(display_feats[d])\n",
    "    d_order.append(d)\n",
    "d_order = np.array(d_order, dtype=np.int32)\n",
    "\n",
    "feats_for_df = np.array(feats_for_df)\n",
    "new_columns = list(range(len(feats_for_df[0])))\n",
    "\n",
    "qdf = pd.DataFrame(data=feats_for_df, columns=new_columns)\n",
    "qdf.insert(loc=0, column='display_id', value=d_order)\n",
    "qdf.set_index('display_id', inplace=True)\n",
    "\n",
    "id = select_features_by_variation(qdf, variation_measure='var', num=num)\n",
    "qdf = qdf.iloc[:, id]\n",
    "\n",
    "norm_data = min_max_transform(qdf.values)\n",
    "norm_data = pd.DataFrame(norm_data, columns=qdf.columns, index=qdf.index)\n",
    "\n",
    "fea_dist_method = 'Euclidean'\n",
    "image_dist_method = 'Euclidean'\n",
    "error = 'abs'\n",
    "result_dir = f'./IGTD/results/display_img'\n",
    "os.makedirs(name=result_dir, exist_ok=True)\n",
    "table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n",
    "                max_step, val_step, result_dir, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_for_df = {}\n",
    "d_order = []\n",
    "for d in display_feats_seperate:\n",
    "    for i in range(len(display_feats_seperate[d])):\n",
    "        if not (i in feats_for_df):\n",
    "            feats_for_df[i] = []\n",
    "        feats_for_df[i].append(display_feats_seperate[d][i])\n",
    "    d_order.append(d)\n",
    "d_order = np.array(d_order, dtype=np.int32)\n",
    "\n",
    "d_list = []\n",
    "for i in feats_for_df:\n",
    "    feats_for_df[i] = np.array(feats_for_df[i])\n",
    "    new_columns = list(range(len(feats_for_df[i][0])))\n",
    "\n",
    "    qdf = pd.DataFrame(data=feats_for_df[i], columns=new_columns)\n",
    "    qdf.insert(loc=0, column='display_id', value=d_order)\n",
    "    qdf.set_index('display_id', inplace=True)\n",
    "\n",
    "    id = select_features_by_variation(qdf, variation_measure='var', num=num)\n",
    "    qdf = qdf.iloc[:, id]\n",
    "\n",
    "    norm_data = min_max_transform(qdf.values)\n",
    "    norm_data = pd.DataFrame(norm_data, columns=qdf.columns, index=qdf.index)\n",
    "    \n",
    "    fea_dist_method = 'Euclidean'\n",
    "    image_dist_method = 'Euclidean'\n",
    "    error = 'abs'\n",
    "    result_dir = f'./IGTD/results/display_img_{i}'\n",
    "    os.makedirs(name=result_dir, exist_ok=True)\n",
    "    table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n",
    "                   max_step, val_step, result_dir, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dist_method = 'Euclidean'\n",
    "# error = 'abs'\n",
    "# result_dir = './IGTD/results/display_images'\n",
    "# os.makedirs(name=result_dir, exist_ok=True)\n",
    "\n",
    "# method_list = ['Euclidean'] * len(d_list)\n",
    "# weight_list = [1 / len(d_list)] * len(d_list)\n",
    "# multi_table_to_image(norm_d_list=d_list, weight_list=weight_list,\n",
    "#                      fea_dist_method_list=method_list, scale=[num_row, num_col],\n",
    "#                      image_dist_method=image_dist_method, save_image_size=save_image_size,\n",
    "#                      max_step=max_step, val_step=val_step, normDir=result_dir, error=error,\n",
    "#                      switch_t=0, min_gain=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the IGTD algorithm using (1) the Euclidean distance for calculating pairwise feature distances and pariwise pixel\n",
    "# # distances and (2) the absolute function for evaluating the difference between the feature distance ranking matrix and\n",
    "# # the pixel distance ranking matrix. Save the result in Test_1 folder.\n",
    "# fea_dist_method = 'Euclidean'\n",
    "# image_dist_method = 'Euclidean'\n",
    "# error = 'abs'\n",
    "# result_dir = './IGTD/results/display_images'\n",
    "# os.makedirs(name=result_dir, exist_ok=True)\n",
    "# table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,\n",
    "#                max_step, val_step, result_dir, error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e802ba4c09f082a495f80f8abdc4fc1e2f9ece889ca9ac1dda9454105a6aae85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
